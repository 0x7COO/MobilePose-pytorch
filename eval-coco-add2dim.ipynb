{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "torch.cuda.set_device(0) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [0,1]\n",
    "\n",
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "\n",
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "\n",
    "        h, w = image_.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image = transform.resize(image_, (new_h, new_w))\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "class Expansion(object): \n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    "        h, w = image.shape[:2]\n",
    "        x = np.arange(0, h)\n",
    "        y = np.arange(0, w) \n",
    "        x, y = np.meshgrid(x, y)\n",
    "        x = x[:,:, np.newaxis]\n",
    "        y = y[:,:, np.newaxis]\n",
    "        image = np.concatenate((image, x), axis=2)\n",
    "        image = np.concatenate((image, y), axis=2)\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = (image[:,:,:3]-mean)/std\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "    \n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "###############################COCO CLASS###############################\n",
    "# define coco class\n",
    "import json\n",
    "import numpy\n",
    "from collections import namedtuple, Mapping\n",
    "\n",
    "# Create namedtuple without defaults\n",
    "def namedtuple_with_defaults(typename, field_names, default_values=()):\n",
    "    T = namedtuple(typename, field_names)\n",
    "    T.__new__.__defaults__ = (None,) * len(T._fields)\n",
    "    if isinstance(default_values, Mapping):\n",
    "        prototype = T(**default_values)\n",
    "    else:\n",
    "        prototype = T(*default_values)\n",
    "    T.__new__.__defaults__ = tuple(prototype)\n",
    "    return T\n",
    "\n",
    "# Used for solving TypeError: Object of type 'float32' is not JSON serializable\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "# Classes for coco groud truth, CocoImage and CocoAnnotation\n",
    "CocoImage = namedtuple_with_defaults('image', ['file_name', 'height', 'width', 'id'])\n",
    "CocoAnnotation = namedtuple_with_defaults('annotation', ['num_keypoints', 'area', \n",
    "                                             'iscrowd', 'keypoints', \n",
    "                                             'image_id', 'bbox', 'category_id',\n",
    "                                            'id'])\n",
    "class CocoData:\n",
    "    def __init__(self, coco_images_arr, coco_annotations_arr):\n",
    "        self.Coco = {}\n",
    "        coco_images_arr = [item._asdict() for item in coco_images_arr]\n",
    "        coco_annotations_arr = [item._asdict() for item in coco_annotations_arr]\n",
    "        self.Coco['images'] = coco_images_arr\n",
    "        self.Coco['annotations'] = coco_annotations_arr\n",
    "        self.Coco['categories'] = [{\"id\": 1, \"name\": \"test\"}]\n",
    "        \n",
    "    def dumps(self):\n",
    "        return json.dumps(self.Coco, cls=MyEncoder) \n",
    "\n",
    "# Change keypoints [x, y, prob] prob = int(prob)\n",
    "def float2int(str_data):\n",
    "    json_data = json.loads(str_data)\n",
    "    annotations = []\n",
    "    if 'annotations' in json_data:\n",
    "        annotations = json_data['annotations']\n",
    "    else:\n",
    "        annotations = json_data\n",
    "    json_size = len(annotations)\n",
    "    for i in range(json_size):\n",
    "        annotation = annotations[i]\n",
    "        keypoints = annotation['keypoints']\n",
    "        keypoints_num = int(len(keypoints) / 3)\n",
    "        for j in range(keypoints_num):\n",
    "            keypoints[j * 3 + 2] = int(round(keypoints[j * 3 + 2]))\n",
    "    return json.dumps(json_data)\n",
    "\n",
    "# Append coco ground truth to coco_images_arr and coco_annotations_arr\n",
    "def transform_to_coco_gt(datas, coco_images_arr, coco_annotations_arr):\n",
    "    \"\"\"\n",
    "    data: num_samples * 32, type Tensor\n",
    "    16 keypoints\n",
    "    \n",
    "    output:\n",
    "    inside coco_images_arr, coco_annotations_arr\n",
    "    \"\"\"\n",
    "    for idx, sample in enumerate(datas):\n",
    "        coco_image = CocoImage()\n",
    "        coco_annotation = CocoAnnotation()\n",
    "        sample = np.array(sample.numpy()).reshape(-1, 2)\n",
    "        num_keypoints = len(sample)    \n",
    "        keypoints = np.append(sample, np.array(np.ones(num_keypoints).reshape(-1, 1) * 2), \n",
    "                      axis=1)\n",
    "        xmin = np.min(sample[:,0])\n",
    "        ymin = np.min(sample[:,1])\n",
    "        xmax = np.max(sample[:,0])\n",
    "        ymax = np.max(sample[:,1])\n",
    "        width = ymax - ymin\n",
    "        height = xmax - xmin\n",
    "        coco_image = coco_image._replace(id = idx, width=width, height=height, file_name=\"\")\n",
    "        coco_annotation = coco_annotation._replace(num_keypoints=num_keypoints)\n",
    "        coco_annotation = coco_annotation._replace(area=width*height)\n",
    "        coco_annotation = coco_annotation._replace(keypoints=keypoints.reshape(-1))\n",
    "        coco_annotation = coco_annotation._replace(image_id=idx)\n",
    "        coco_annotation = coco_annotation._replace(bbox=[xmin, ymin, width, height])\n",
    "        coco_annotation = coco_annotation._replace(category_id=1) # default \"1\" for keypoint\n",
    "        coco_annotation = coco_annotation._replace(id=idx)\n",
    "        coco_annotation = coco_annotation._replace(iscrowd=0)\n",
    "        coco_images_arr.append(coco_image)\n",
    "        coco_annotations_arr.append(coco_annotation)\n",
    "    return ()\n",
    "\n",
    "# Coco predict result class\n",
    "CocoPredictAnnotation = namedtuple_with_defaults('predict_anno', ['image_id', 'category_id', 'keypoints', 'score'])\n",
    "\n",
    "# Append coco predict result to coco_images_arr and coco_pred_annotations_arr\n",
    "def transform_to_coco_pred(datas, coco_pred_annotations_arr, beg_idx):\n",
    "    \"\"\"\n",
    "    data: num_samples * 32, type Variable\n",
    "    16 keypoints\n",
    "    \n",
    "    output:\n",
    "    inside coco_pred_annotations_arr\n",
    "    \"\"\"\n",
    "    for idx, sample in enumerate(datas):\n",
    "        coco_pred_annotation = CocoPredictAnnotation()\n",
    "        \n",
    "        sample = np.array(sample.data.cpu().numpy()).reshape(-1, 2)\n",
    "        num_keypoints = len(sample)        \n",
    "        keypoints = np.append(sample, np.array(np.ones(num_keypoints).reshape(-1, 1) * 2), \n",
    "                      axis=1)\n",
    "        xmin = np.min(sample[:,0])\n",
    "        ymin = np.min(sample[:,1])\n",
    "        xmax = np.max(sample[:,0])\n",
    "        ymax = np.max(sample[:,1])\n",
    "        width = ymax - ymin\n",
    "        height = xmax - xmin\n",
    "        # set value\n",
    "        cur_idx = beg_idx + idx\n",
    "        coco_pred_annotation = coco_pred_annotation._replace(image_id=cur_idx)\n",
    "        coco_pred_annotation = coco_pred_annotation._replace(category_id=1)\n",
    "        coco_pred_annotation = coco_pred_annotation._replace(keypoints=keypoints.reshape(-1))\n",
    "        coco_pred_annotation = coco_pred_annotation._replace(score=2)\n",
    "        # add to arr\n",
    "        coco_pred_annotations_arr.append(coco_pred_annotation)\n",
    "    return ()\n",
    "\n",
    "###############################NET###############################\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing dataset, wait...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae54282232e94353a2c63df211afb4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "print(\"Loading testing dataset, wait...\")\n",
    "# load dataset\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                              transform=transforms.Compose([\n",
    "                                           Rescale((227,227)),\n",
    "                                           Expansion(),\n",
    "                                           ToTensor()\n",
    "                                       ]))\n",
    "test_dataset_size = len(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_dataset_size,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "# get all test data\n",
    "all_test_data = {}\n",
    "for i_batch, sample_batched in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    all_test_data = sample_batched\n",
    "    \n",
    "def eval_coco(net_path, result_gt_json_path, result_pred_json_path):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    eval_coco('/home/yuliang/code/PoseFlow/checkpoint140.t7', \n",
    "    'result-gt-json.txt', 'result-pred-json.txt')\n",
    "    \"\"\"\n",
    "    # load net\n",
    "    net = Net().cuda(device_id=gpus[0])\n",
    "    criterion = nn.MSELoss().cuda(device_id=gpus[0])\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
    "    net = torch.load(net_path).cuda(device_id=gpus[0])\n",
    "    ##### generate result ground truth json #####\n",
    "    total_size = len(all_test_data['image'])\n",
    "    all_coco_images_arr = [] \n",
    "    all_coco_annotations_arr = []\n",
    "    transform_to_coco_gt(all_test_data['pose'], all_coco_images_arr, all_coco_annotations_arr)\n",
    "    coco = CocoData(all_coco_images_arr, all_coco_annotations_arr)\n",
    "    coco_str =  coco.dumps()\n",
    "    result_gt_json = float2int(coco_str)\n",
    "    # save ground truth json to file\n",
    "    f = open(result_gt_json_path, \"w\")\n",
    "    f.write(result_gt_json)\n",
    "    f.close()\n",
    "    ##### generate result ground truth json #####\n",
    "    total_size = len(all_test_data['image'])\n",
    "    all_coco_pred_annotations_arr = [] \n",
    "    for i in range(1, ceil(total_size / 100.0) + 1):\n",
    "        sample_data = {}\n",
    "        print(100 * (i - 1), min(100 * i, total_size))\n",
    "        sample_data['image'] = all_test_data['image'][100 * (i - 1) : min(100 * i, total_size)].cuda(device=gpus[0])\n",
    "        output = net(Variable(sample_data['image'],volatile=True))\n",
    "        transform_to_coco_pred(output, all_coco_pred_annotations_arr, 100 * (i - 1))\n",
    "\n",
    "    all_coco_pred_annotations_arr = [item._asdict() for item in all_coco_pred_annotations_arr]\n",
    "    result_pred_json = json.dumps(all_coco_pred_annotations_arr, cls=MyEncoder)\n",
    "    result_pred_json = float2int(result_pred_json)\n",
    "    # save result predict json to file\n",
    "    f = open(result_pred_json_path, \"w\")\n",
    "    f.write(result_pred_json)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n",
      "0 100\n",
      "100 200\n",
      "200 300\n",
      "300 400\n",
      "400 500\n",
      "500 600\n",
      "600 700\n",
      "700 800\n",
      "800 900\n",
      "900 1000\n",
      "1000 1100\n",
      "1100 1200\n",
      "1200 1300\n",
      "1300 1400\n",
      "1400 1500\n",
      "1500 1600\n",
      "1600 1700\n",
      "1700 1800\n",
      "1800 1900\n",
      "1900 1991\n"
     ]
    }
   ],
   "source": [
    "#eval_coco(net_path, result_gt_json_path, result_pred_json_path)\n",
    "PATH_PREFIX = \"./cocoapi/PythonAPI/txts/add2dim\"\n",
    "if not os.path.exists(PATH_PREFIX):\n",
    "    os.makedirs(PATH_PREFIX)\n",
    "mdir=\"/disk3/yinghong/data/mobile-model\"\n",
    "for i in range(0,300, 10):\n",
    "    filename = \"add2dim-checkpoint{}.t7\".format(i)\n",
    "    full_name = os.path.join(mdir, filename)\n",
    "    eval_coco(full_name, \\\n",
    "    os.path.join(PATH_PREFIX, 'result-gt-add2dim-{}-json.txt'.format(i)), \n",
    "    os.path.join(PATH_PREFIX, 'result-pred-add2dim-{}-json.txt'.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
