{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "# from multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# torch.cuda.set_device(0) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "def display_pose(img, pose):\n",
    "    pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "    img = img.cpu().numpy().transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "\n",
    "        h, w = image_.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image = transform.resize(image_, (new_h, new_w))\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = torch.from_numpy(((image-mean)/std).transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        \n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'train_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=256,\n",
    "#                         shuffle=False, num_workers = 10)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b604ac342209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnet_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH_PREFIX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'checkpoint{}.t7'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# net = torch.load(net_path).cuda(device_id=gpus[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b604ac342209>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n",
    "\n",
    "# OLD_VERSION = 8\n",
    "PATH_PREFIX = '/home/yuliang/code/DeepPose-pytorch-yh/checkpoint/'\n",
    "net_path = PATH_PREFIX + 'checkpoint{}.t7'.format(140)\n",
    "# net = torch.load(net_path).cuda(device_id=gpus[0])\n",
    "net = Net()\n",
    "net = torch.nn.DataParallel(net).cuda()\n",
    "\n",
    "# net = Net()\n",
    "criterion = nn.MSELoss().cuda(device_id=gpus[0])\n",
    "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1c42c8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from scipy import misc\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import parameters as iap\n",
    "plt.figure(figsize=(20,140))\n",
    "\n",
    "def pose2keypoints(images, poses):\n",
    "    keypoints_on_images = []\n",
    "    for img_idx, (image, pose) in enumerate(zip(images, poses)):\n",
    "        height, width = image.shape[0:2]\n",
    "        keypoints = []\n",
    "        for i in range(int(poses.shape[1] / 2)):\n",
    "            x = int(poses[img_idx][i * 2])\n",
    "            y = int(poses[img_idx][i * 2 + 1])\n",
    "            keypoints.append(ia.Keypoint(x=x, y=y))\n",
    "        keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape))\n",
    "    return keypoints_on_images\n",
    "\n",
    "def keypoints2pose(keypoints_aug):\n",
    "    all_keypoints = []\n",
    "    for keypoints_after in keypoints_aug:\n",
    "        one_person = []\n",
    "        for kp_idx, keypoint in enumerate(keypoints_after.keypoints):\n",
    "            x_new, y_new = keypoint.x, keypoint.y\n",
    "            one_person.append(np.array(x_new).astype(np.float32))\n",
    "            one_person.append(np.array(y_new).astype(np.float32))\n",
    "        all_keypoints.append(np.array(one_person))\n",
    "    return np.array(all_keypoints)    \n",
    "\n",
    "def img_augmentation(images, poses, seq_dets):\n",
    "    \"\"\"\n",
    "    images, poses :tensor;\n",
    "    seq_dets: [iaa.Sequential]\n",
    "    Example:\n",
    "    seqs = [iaa.Sequential([iaa.Affine(rotate=45)]), iaa.Sequential([iaa.Fliplr(1.0)])]\n",
    "    seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "    result_images, result_poses = img_augmentation(images, poses, seq_dets)\n",
    "    \"\"\"\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    glocal_cnt = 0\n",
    "    np_images = images.numpy()\n",
    "    np_poses = poses.numpy()\n",
    "    out_images = copy.deepcopy(np_images)\n",
    "    out_poses = copy.deepcopy(np_poses)\n",
    "    # recover \n",
    "#     for i in range(np_images.shape[0]):\n",
    "#         np_images[i] = np_images[i] * std + mean\n",
    "    \n",
    "    np_images = np.transpose(np_images, (0, 2, 3, 1))\n",
    "    keypoints_on_images = pose2keypoints(np_images, np_poses)\n",
    "    seq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
    "    seq_det = seq.to_deterministic() \n",
    "    for seq_det in seq_dets:\n",
    "#         print(\"!!!!!!!!!!\", np_images.shape)\n",
    "#         np_images = np_images * std + mean\n",
    "        images_aug = seq_det.augment_images(np_images)\n",
    "#         images_aug = (images_aug - mean) / std\n",
    "        keypoints_aug = seq_det.augment_keypoints(keypoints_on_images)\n",
    "        new_keypoints = keypoints2pose(keypoints_aug)\n",
    "        \n",
    "        images_aug = np.transpose(images_aug, (0, 3, 1, 2)).astype(np.float32)\n",
    "#         plt.clf()\n",
    "#         for i in range(8):\n",
    "#             display_pose2(images_aug[i], new_keypoints[i], glocal_cnt) \n",
    "#             glocal_cnt += 1\n",
    "#         print(new_keypoints)\n",
    "#         images_aug = np.transpose(images_aug, (0, 2, 3, 1))\n",
    "\n",
    "#         print(out_images.shape, np_images.shape)\n",
    "        \n",
    "#         print(np_images.shape)    \n",
    "        out_images = np.vstack((out_images, images_aug))\n",
    "        out_poses = np.vstack((out_poses, new_keypoints))\n",
    "#         print(\"out_images: \", out_images.shape)\n",
    "    # force to float\n",
    "    # change to tensor\n",
    "    images = torch.from_numpy(out_images)\n",
    "    poses = torch.from_numpy(out_poses)\n",
    "    return (images, poses)\n",
    "\n",
    "# seqs = [iaa.Sequential([iaa.Affine(rotate=45)]), iaa.Sequential([iaa.Fliplr(1.0)])]\n",
    "# # call this for each batch again, NOT only once at the start\n",
    "# seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "# result_images, result_poses = img_augmentation(images, poses, seq_dets)\n",
    "# print(result_images.shape, result_poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a86d15624174496866b0963e2231b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 100\n",
      "i: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from scipy import misc\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import parameters as iap\n",
    "import time\n",
    "import numpy\n",
    "# import gzip\n",
    "\n",
    "# plt.figure(figsize=(20,140))\n",
    "\n",
    "# aug_data = []\n",
    "# max_aug_data_idx = 0\n",
    "\n",
    "SAVE_PATH = '/disk3/yinghong/data/mobile-aug-data/'\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "        \n",
    "for i, data in tqdm_notebook(enumerate(train_dataloader)):\n",
    "#     print(\"go!!!\")\n",
    "    if i % 100 == 0:\n",
    "        print(\"i: %d\" % i)\n",
    "    images, poses = data['image'], data['pose']\n",
    "    \n",
    "    max_aug_data_idx = i\n",
    "    seqs = []\n",
    "    time1 = time.time()\n",
    "#     for rotate in range(-180, 180, 45):\n",
    "#         seqs.append(iaa.Sequential([iaa.Affine(rotate=rotate)]))\n",
    "#     for scale_num in range(3, 8, 2):\n",
    "#         seqs.append(iaa.Sequential([iaa.Affine(scale=(scale_num / 10.0 , scale_num / 10.0 + 0.1))]))\n",
    "\n",
    "    seqs.append(iaa.Sequential([iaa.Fliplr(0.5)]))\n",
    "#     seqs.append(iaa.Sequential([iaa.Flipud(1.0)]))\n",
    "\n",
    "#     seqs.append(iaa.GaussianBlur(sigma=(0, 2)))\n",
    "#     seqs.append(iaa.GaussianBlur(sigma=(0, 4)))\n",
    "#     seqs.append(iaa.GaussianBlur(sigma=(0, 6)))\n",
    "    seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "# #     print(\"pre images,\", images[0])\n",
    "    images, poses = img_augmentation(images, poses, seq_dets)\n",
    "    \n",
    "\n",
    "#     print(\"post images,\", images[0])\n",
    "    time2 = time.time()\n",
    "    obj = {'images': images, 'poses': poses, 'idx': i, 'batch_size': BATCH_SIZE }\n",
    "    time3 = time.time()\n",
    "    torch.save(obj, SAVE_PATH + \"aug-data-same-scale-0.5lr-type-bz%d-idx%d\" % (BATCH_SIZE, i))\n",
    "#     print(\"transform time=%d, save time=%d\" % (time2 - time1, time3 - time2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "plt.figure(figsize=(20,140))\n",
    "\n",
    "SAVE_PATH = \"/disk3/yinghong/data/mobile-aug-data/\"\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum(torch.pow(input - target,2)) / input.nelement()\n",
    "\n",
    "train_loss_all = []\n",
    "valid_loss_all = []\n",
    "# TBD(@yinghong): delete it after use\n",
    "def display_pose(img, pose, ids):\n",
    "    print\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "#     pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "#     pose  = pose.numpy().reshape([-1,2])\n",
    "    pose  = pose.numpy().reshape([-1,2])\n",
    "#     img = img.cpu().numpy().transpose(1,2,0)\n",
    "#     img = img.numpy().transpose(1,2,0)\n",
    "    img = img.numpy().transpose(1,2,0)\n",
    "    colors = ['b', 'g', 'g', 'g', 'g', 'g', 'm', 'm', 'r', 'r', 'y', 'y', 'y', 'y','y','y']\n",
    "    pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n",
    "    colors_skeleton = ['r', 'y', 'y', 'g', 'g', 'y', 'y', 'g', 'g', 'm', 'm', 'g', 'g', 'y','y']\n",
    "\n",
    "    img = img*std+mean\n",
    "#     print(img.shape) # (227, 227, 3)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    plt.subplot(25,4,ids+1)\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(len(colors)):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color=colors[idx])\n",
    "    for idx in range(len(colors_skeleton)):\n",
    "        plt.plot(pose[pairs[idx],0], pose[pairs[idx],1],color=colors_skeleton[idx])\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=1))\n",
    "output_dict = {}\n",
    "\n",
    "MAX_BATCH_SIZE = 561\n",
    "IMG_WIDTH=227\n",
    "IMG_HEIGHT=227\n",
    "for epoch in tqdm_notebook(range(1000)):  # loop over the dataset multiple times\n",
    "    train_loss_epoch = []\n",
    "    beg_time = time.time()\n",
    "#     for i, data in enumerate(train_dataloader):\n",
    "    for i in tqdm_notebook(range(MAX_BATCH_SIZE)):\n",
    "        if i % 100 == 0:\n",
    "            print(\"current=%d\" % i)\n",
    "        # get the inputs\n",
    "       \n",
    "#         images, poses = data['image'], data['pose']\n",
    "        obj = torch.load(SAVE_PATH + \"aug-data-same-scale-bz32-idx%d\" % i)\n",
    "        images = obj['images']\n",
    "#         print(\"images.size,\", images[0])\n",
    "        poses = obj['poses']\n",
    "#         print(\"before:\", images.shape, poses.shape)\n",
    "#         print(\"before:\", type(images), type(poses))\n",
    "   \n",
    "#         seqs = [iaa.Crop(px=(0, 16))]\n",
    "        \n",
    "###########data aug#############\n",
    "#         seqs = []\n",
    "#         for rotate in range(-180, 180, 45):\n",
    "#             seqs.append(iaa.Sequential([iaa.Affine(rotate=rotate)]))\n",
    "#         for scale_num in range(3, 8, 2):\n",
    "#             seqs.append(iaa.Sequential([iaa.Affine(scale=(scale_num / 10.0 , scale_num / 10.0 + 0.1))]))\n",
    "#         seqs.append(iaa.Sequential([iaa.Fliplr(1.0)]))\n",
    "# #         seqs.append(iaa.Sequential([iaa.Flipud(1.0)]))\n",
    "# #         seqs.append(iaa.GaussianBlur(sigma=(0, 1.0)))\n",
    "#         seqs.append(iaa.GaussianBlur(sigma=(0, 2.0)))\n",
    "# #         seqs.append(iaa.GaussianBlur(sigma=(0, 3.0)))\n",
    "\n",
    "#         seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "#         images, poses = img_augmentation(images, poses, seq_dets)\n",
    "#         print(\"after:\", images.shape, poses.shape)\n",
    "#         for i in range(4):\n",
    "#             print(\"begin drawing\")\n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[i], poses[i], i)\n",
    "#             plt.show()\n",
    "# #             print(poses[i])\n",
    "            \n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[24 + i], poses[24 + i], i)\n",
    "#             plt.show()\n",
    "#             print(poses[24 + i])\n",
    "            \n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[64 + i], poses[64 + i], i)\n",
    "#             plt.show()\n",
    "#             print(poses[128 + i])\n",
    "#         print(\"before:\", type(images), type(poses))\n",
    "\n",
    "#         if i == 0:\n",
    "# #             np.save(outfile, [images, poses])\n",
    "#             pickle.dump(output_dict, open(\"output_dict.txt\", \"wb\"))\n",
    "#             print(\"dump done\")\n",
    "\n",
    "        # wrap them in Variable\n",
    "        \n",
    "        images, poses = Variable(images.cuda()), Variable(poses.cuda())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        # TBD(@yinghong): change to a function\n",
    "#         print(type(outputs[0][0]), type(poses[0][0]))\n",
    "#         print(outputs[0][0].data[0])\n",
    "\n",
    "#         for pose_idx in range(poses.size()[0]):\n",
    "#             x = poses[pose_idx][0].data[0]\n",
    "#             y = poses[pose_idx][1].data[0]\n",
    "#             if x < 0 or x < 0 or y >= IMG_WIDTH or y >= IMG_HEIGHT:\n",
    "#                 poses[pose_idx][0].data[0] = outputs[pose_idx][0].data[0]\n",
    "#                 poses[pose_idx][1].data[0] = outputs[pose_idx][1].data[0]\n",
    "        loss = criterion(outputs, poses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_epoch.append(loss.data[0])\n",
    "    print(\"epoch %d\", epoch)\n",
    "    end_time = time.time()\n",
    "    if epoch%3==0:\n",
    "        PATH_PREFIX = './'\n",
    "        checkpoint_file = PATH_PREFIX + 'aug-data-same-scale-bz32-checkpoint{}.t7'.format(1 + epoch)\n",
    "        torch.save(net, checkpoint_file)\n",
    "        valid_loss_epoch = []\n",
    "        for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "\n",
    "            net_forward = torch.load(checkpoint_file).cuda(device_id=gpus[0])\n",
    "            images = sample_batched['image'].cuda(device=gpus[0])\n",
    "            poses = sample_batched['pose'].cuda(device=gpus[0])\n",
    "            outputs = net_forward(Variable(images, volatile=True))\n",
    "            valid_loss_epoch.append(mse_loss(outputs.data,poses))\n",
    "        print('[epoch %d] train loss: %.8f, valid loss: %.8f' %\n",
    "          (epoch + 1, sum(train_loss_epoch)/(71*256), sum(valid_loss_epoch)/(8*256)))\n",
    "        print('==> checkpoint model saving to %s'%checkpoint_file)\n",
    "        train_loss_all.append(sum(train_loss_epoch)/(71*256))\n",
    "        valid_loss_all.append(sum(valid_loss_epoch)/(8*256))\n",
    "    print(\"Time:%d\", end_time - beg_time)\n",
    "            \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict = pickle.load(open(\"output_dict.txt\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_dict['images'].shape)\n",
    "# print(output_dict['poses'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# output_dict = pickle.load(open(\"output_dict.txt\", \"rb\"))\n",
    "# print(output_dict['images'].shape)\n",
    "# print(output_dict['poses'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_dict['poses'].numpy()[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# x1 = torch.zeros(10, 10)\n",
    "# x2 = x1.unsqueeze(0)\n",
    "# print(x2.size())\n",
    "# sz=torch.Size([1, 10, 10])\n",
    "# print(sz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
