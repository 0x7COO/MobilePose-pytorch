{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "# from multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "torch.cuda.set_device(0) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "def display_pose(img, pose):\n",
    "    pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "    img = img.cpu().numpy().transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))\n",
    "\n",
    "# TBD(@yinghong): remove it after test\n",
    "def display_pose_tmp(img, pose):\n",
    "    pose  = pose.reshape([-1,2])\n",
    "#     img = img.transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         image_, pose_ = sample['image'], sample['pose']\n",
    "#         h, w = image_.shape[:2]\n",
    "#         im_scale = min(float(self.output_size[0]) / float(h), float(self.output_size[1]) / float(w))\n",
    "#         new_h = int(image_.shape[0] * im_scale)\n",
    "#         new_w = int(image_.shape[1] * im_scale)\n",
    "#         image = cv2.resize(image_, (new_w, new_h),\n",
    "#                     interpolation=cv2.INTER_LINEAR)\n",
    "#         left_pad = (self.output_size[1] - new_w) // 2\n",
    "#         right_pad = (self.output_size[1] - new_w) - left_pad\n",
    "#         top_pad = (self.output_size[0] - new_h) // 2\n",
    "#         bottom_pad = (self.output_size[0] - new_h) - top_pad\n",
    "#         mean=np.array([0.485, 0.456, 0.406]) * 256\n",
    "#         pad = ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "#         image = np.stack([np.pad(image[:,:,c], pad, mode='constant', constant_values=mean[c]) \n",
    "#                         for c in range(3)], axis=2)\n",
    "#         pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h]))\n",
    "#         pose += [right_pad, top_pad]\n",
    "#         pose = pose.flatten()\n",
    "#         return {'image': image, 'pose': pose}\n",
    "        \n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "        h, w = image_.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        image = transform.resize(image_, (new_h, new_w))\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "        display_pose_tmp(image, pose)\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = (image[:,:,:3]-mean)/std\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "    \n",
    "class Expansion(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    "        h, w = image.shape[:2]\n",
    "        x = np.arange(0, h)\n",
    "        y = np.arange(0, w) \n",
    "        x, y = np.meshgrid(x, y)\n",
    "        x = x[:,:, np.newaxis]\n",
    "        y = y[:,:, np.newaxis]\n",
    "        image = np.concatenate((image, x), axis=2)\n",
    "        image = np.concatenate((image, y), axis=2)\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        \n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'train_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "#                                                Expansion(),\n",
    "                                               ToTensor(), \n",
    "                                           ]))\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=256,\n",
    "#                         shuffle=False, num_workers = 10)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "#                                                Expansion(),\n",
    "                                               ToTensor(),\n",
    "                                           ]))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n",
    "\n",
    "# OLD_VERSION = 8\n",
    "PATH_PREFIX = '/home/yuliang/code/DeepPose-pytorch-yh/checkpoint/'\n",
    "net_path = PATH_PREFIX + 'checkpoint{}.t7'.format(140)\n",
    "# net = torch.load(net_path).cuda(device_id=gpus[0])\n",
    "net = Net()\n",
    "# net = torch.nn.DataParallel(net).cuda()\n",
    "\n",
    "# net = Net()\n",
    "criterion = nn.MSELoss().cuda(device_id=gpus[0])\n",
    "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efffad60048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from scipy import misc\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import parameters as iap\n",
    "plt.figure(figsize=(20,140))\n",
    "\n",
    "def pose2keypoints(images, poses):\n",
    "    keypoints_on_images = []\n",
    "    for img_idx, (image, pose) in enumerate(zip(images, poses)):\n",
    "        height, width = image.shape[0:2]\n",
    "        keypoints = []\n",
    "        for i in range(int(poses.shape[1] / 2)):\n",
    "            x = int(poses[img_idx][i * 2])\n",
    "            y = int(poses[img_idx][i * 2 + 1])\n",
    "            keypoints.append(ia.Keypoint(x=x, y=y))\n",
    "        keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape))\n",
    "    return keypoints_on_images\n",
    "\n",
    "def keypoints2pose(keypoints_aug):\n",
    "    all_keypoints = []\n",
    "    for keypoints_after in keypoints_aug:\n",
    "        one_person = []\n",
    "        for kp_idx, keypoint in enumerate(keypoints_after.keypoints):\n",
    "            x_new, y_new = keypoint.x, keypoint.y\n",
    "            one_person.append(np.array(x_new).astype(np.float32))\n",
    "            one_person.append(np.array(y_new).astype(np.float32))\n",
    "        all_keypoints.append(np.array(one_person))\n",
    "    return np.array(all_keypoints)    \n",
    "\n",
    "def img_augmentation(images, poses, seq_dets):\n",
    "    \"\"\"\n",
    "    images, poses :tensor;\n",
    "    seq_dets: [iaa.Sequential]\n",
    "    Example:\n",
    "    seqs = [iaa.Sequential([iaa.Affine(rotate=45)]), iaa.Sequential([iaa.Fliplr(1.0)])]\n",
    "    seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "    result_images, result_poses = img_augmentation(images, poses, seq_dets)\n",
    "    \"\"\"\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    glocal_cnt = 0\n",
    "    np_images = images.numpy()\n",
    "    np_poses = poses.numpy()\n",
    "    out_images = copy.deepcopy(np_images)\n",
    "    out_poses = copy.deepcopy(np_poses)\n",
    "    # recover \n",
    "#     for i in range(np_images.shape[0]):\n",
    "#         np_images[i] = np_images[i] * std + mean\n",
    "    \n",
    "    np_images = np.transpose(np_images, (0, 2, 3, 1))\n",
    "    keypoints_on_images = pose2keypoints(np_images, np_poses)\n",
    "    seq = iaa.Sequential([iaa.Fliplr(1.0)])\n",
    "    seq_det = seq.to_deterministic() \n",
    "    for seq_det in seq_dets:\n",
    "#         print(\"!!!!!!!!!!\", np_images.shape)\n",
    "#         np_images = np_images * std + mean\n",
    "        images_aug = seq_det.augment_images(np_images)\n",
    "#         images_aug = (images_aug - mean) / std\n",
    "        keypoints_aug = seq_det.augment_keypoints(keypoints_on_images)\n",
    "        new_keypoints = keypoints2pose(keypoints_aug)\n",
    "        \n",
    "        images_aug = np.transpose(images_aug, (0, 3, 1, 2)).astype(np.float32)\n",
    "#         plt.clf()\n",
    "#         for i in range(8):\n",
    "#             display_pose2(images_aug[i], new_keypoints[i], glocal_cnt) \n",
    "#             glocal_cnt += 1\n",
    "#         print(new_keypoints)\n",
    "#         images_aug = np.transpose(images_aug, (0, 2, 3, 1))\n",
    "\n",
    "#         print(out_images.shape, np_images.shape)\n",
    "        \n",
    "#         print(np_images.shape)    \n",
    "        out_images = np.vstack((out_images, images_aug))\n",
    "        out_poses = np.vstack((out_poses, new_keypoints))\n",
    "#         print(\"out_images: \", out_images.shape)\n",
    "    # force to float\n",
    "    # change to tensor\n",
    "    images = torch.from_numpy(out_images)\n",
    "    poses = torch.from_numpy(out_poses)\n",
    "    return (images, poses)\n",
    "\n",
    "# seqs = [iaa.Sequential([iaa.Affine(rotate=45)]), iaa.Sequential([iaa.Fliplr(1.0)])]\n",
    "# # call this for each batch again, NOT only once at the start\n",
    "# seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "# result_images, result_poses = img_augmentation(images, poses, seq_dets)\n",
    "# print(result_images.shape, result_poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import json\n",
    "# from tqdm import tqdm_notebook\n",
    "# import imgaug as ia\n",
    "# from imgaug import augmenters as iaa\n",
    "# from scipy import misc\n",
    "# import copy\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from imgaug import parameters as iap\n",
    "# import time\n",
    "# import numpy\n",
    "# # import gzip\n",
    "\n",
    "# # plt.figure(figsize=(20,140))\n",
    "\n",
    "# # aug_data = []\n",
    "# # max_aug_data_idx = 0\n",
    "\n",
    "# SAVE_PATH = '/disk3/yinghong/data/mobile-aug-data/'\n",
    "\n",
    "# class MyEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, numpy.integer):\n",
    "#             return int(obj)\n",
    "#         elif isinstance(obj, numpy.floating):\n",
    "#             return float(obj)\n",
    "#         elif isinstance(obj, numpy.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         else:\n",
    "#             return super(MyEncoder, self).default(obj)\n",
    "        \n",
    "# for i, data in tqdm_notebook(enumerate(train_dataloader)):\n",
    "# #     print(\"go!!!\")\n",
    "#     if i % 100 == 0:\n",
    "#         print(\"i: %d\" % i)\n",
    "#     images, poses = data['image'], data['pose']\n",
    "    \n",
    "#     max_aug_data_idx = i\n",
    "#     seqs = []\n",
    "#     time1 = time.time()\n",
    "# #     for rotate in range(-180, 180, 45):\n",
    "# #         seqs.append(iaa.Sequential([iaa.Affine(rotate=rotate)]))\n",
    "# #     for scale_num in range(3, 8, 2):\n",
    "# #         seqs.append(iaa.Sequential([iaa.Affine(scale=(scale_num / 10.0 , scale_num / 10.0 + 0.1))]))\n",
    "\n",
    "#     seqs.append(iaa.Sequential([iaa.Fliplr(0.5)]))\n",
    "# #     seqs.append(iaa.Sequential([iaa.Flipud(1.0)]))\n",
    "\n",
    "# #     seqs.append(iaa.GaussianBlur(sigma=(0, 2)))\n",
    "# #     seqs.append(iaa.GaussianBlur(sigma=(0, 4)))\n",
    "# #     seqs.append(iaa.GaussianBlur(sigma=(0, 6)))\n",
    "#     seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "# # #     print(\"pre images,\", images[0])\n",
    "#     images, poses = img_augmentation(images, poses, seq_dets)\n",
    "    \n",
    "\n",
    "# #     print(\"post images,\", images[0])\n",
    "#     time2 = time.time()\n",
    "#     obj = {'images': images, 'poses': poses, 'idx': i, 'batch_size': BATCH_SIZE }\n",
    "#     time3 = time.time()\n",
    "#     torch.save(obj, SAVE_PATH + \"aug-data-same-scale-0.5lr-type-bz%d-idx%d\" % (BATCH_SIZE, i))\n",
    "# #     print(\"transform time=%d, save time=%d\" % (time2 - time1, time3 - time2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4b96e389884b649f0ee387213591ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "current=0\n",
      "epoch 0\n",
      "[epoch 1] train loss: 0.00000000, valid loss: 234.72869246\n",
      "==> checkpoint model saving to /disk3/yinghong/data/mobile-model/orig-checkpoint1.t7\n",
      "Time:%d 10.180804014205933\n",
      "current=0\n",
      "epoch 1\n",
      "Time:%d 8.317771434783936\n",
      "current=0\n",
      "epoch 2\n",
      "Time:%d 13.710424184799194\n",
      "current=0\n",
      "epoch 3\n",
      "Time:%d 18.69949197769165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-91:\n",
      "Process Process-88:\n",
      "Process Process-84:\n",
      "Process Process-75:\n",
      "Process Process-83:\n",
      "Process Process-90:\n",
      "Process Process-86:\n",
      "Process Process-85:\n",
      "Process Process-92:\n",
      "Process Process-81:\n",
      "Process Process-82:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-72:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-76:\n",
      "Process Process-87:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-89:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-77:\n",
      "Process Process-74:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process Process-79:\n",
      "Traceback (most recent call last):\n",
      "Process Process-78:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-80:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 94, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 94, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 94, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 94, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 111, in pil_to_ndarray\n",
      "    frame = np.array(frame, dtype=dtype)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 94, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 41, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2336, in autoscale_view\n",
      "    'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"<ipython-input-4-7ed48bb422ee>\", line 43, in __call__\n",
      "    display_pose_tmp(image, pose)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 33, in display_pose_tmp\n",
      "    plt.imshow(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 90, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2432)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2285, in handle_single_axis\n",
      "    finite_dl = [d for d in dl if np.isfinite(d).all()]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1407, in plot\n",
      "    self.add_line(line)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 620, in ascontiguousarray\n",
      "    return array(a, dtype, copy=False, order='C', ndmin=1)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2266, in autoscale_view\n",
      "    stickies = [artist.sticky_edges for artist in self.get_children()]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2339, in autoscale_view\n",
      "    'minposy', self.yaxis, self._ymargin, y_stickies, self.set_ybound)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3157, in imshow\n",
      "    **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2268, in autoscale_view\n",
      "    y_stickies = sum([sticky.y for sticky in stickies], [])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 1791, in add_line\n",
      "    line.set_clip_path(self.patch)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2266, in <listcomp>\n",
      "    stickies = [artist.sticky_edges for artist in self.get_children()]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2285, in <listcomp>\n",
      "    finite_dl = [d for d in dl if np.isfinite(d).all()]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/artist.py\", line 694, in set_clip_path\n",
      "    path.get_transform())\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/_methods.py\", line 41, in _all\n",
      "    return umr_all(a, axis, dtype, out, keepdims)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/artist.py\", line 951, in sticky_edges\n",
      "    return self._sticky_edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2318, in handle_single_axis\n",
      "    do_lower_margin = not np.any(np.isclose(x0, stickies))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-2-7d469cd0e2d4>\", line 35, in display_pose_tmp\n",
      "    plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 221, in get_transform\n",
      "    return self.get_patch_transform() + artist.Artist.get_transform(self)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2336, in autoscale_view\n",
      "    'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 5128, in imshow\n",
      "    im.set_clip_path(self.patch)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2268, in autoscale_view\n",
      "    y_stickies = sum([sticky.y for sticky in stickies], [])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 3317, in plot\n",
      "    ret = ax.plot(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 721, in get_patch_transform\n",
      "    self._update_patch_transform()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2314, in handle_single_axis\n",
      "    transform = axis.get_transform()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/artist.py\", line 694, in set_clip_path\n",
      "    path.get_transform())\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2268, in <listcomp>\n",
      "    y_stickies = sum([sticky.y for sticky in stickies], [])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 716, in _update_patch_transform\n",
      "    rot_trans.rotate_deg_around(x, y, self._angle)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1898, in inner\n",
      "    return func(ax, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2267, in autoscale_view\n",
      "    x_stickies = sum([sticky.x for sticky in stickies], [])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axis.py\", line 703, in get_transform\n",
      "    return self._scale.get_transform()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1410, in plot\n",
      "    self.autoscale_view(scalex=scalex, scaley=scaley)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 221, in get_transform\n",
      "    return self.get_patch_transform() + artist.Artist.get_transform(self)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py\", line 1407, in plot\n",
      "    self.add_line(line)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 1955, in rotate_deg_around\n",
      "    return self.translate(-x, -y).rotate_deg(degrees).translate(x, y)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2267, in <listcomp>\n",
      "    x_stickies = sum([sticky.x for sticky in stickies], [])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2339, in autoscale_view\n",
      "    'minposy', self.yaxis, self._ymargin, y_stickies, self.set_ybound)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 1793, in add_line\n",
      "    self._update_line_limits(line)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 1848, in _update_line_limits\n",
      "    self.transData)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 1262, in contains_branch_seperately\n",
      "    return [self.contains_branch(other_transform)] * 2\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 2540, in isclose\n",
      "    if all(xfin) and all(yfin):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/scale.py\", line 84, in get_transform\n",
      "    return IdentityTransform()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 721, in get_patch_transform\n",
      "    self._update_patch_transform()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 2050, in all\n",
      "    return arr.all(axis=axis, out=out, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 1663, in __init__\n",
      "    Transform.__init__(self, *args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 99, in __init__\n",
      "    self._parents = WeakValueDictionary()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/_methods.py\", line 41, in _all\n",
      "    return umr_all(a, axis, dtype, out, keepdims)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/weakref.py\", line 107, in __init__\n",
      "    if len(args) > 1:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2318, in handle_single_axis\n",
      "    do_lower_margin = not np.any(np.isclose(x0, stickies))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 1241, in contains_branch\n",
      "    for _, sub_tree in self._iter_break_from_left_to_right():\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 2345, in _iter_break_from_left_to_right\n",
      "    yield lh_compliment, rh_compliment + self._b\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 1179, in __add__\n",
      "    return composite_transform_factory(self, other)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 2495, in composite_transform_factory\n",
      "    return CompositeGenericTransform(a, b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 2540, in isclose\n",
      "    if all(xfin) and all(yfin):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 2307, in __init__\n",
      "    Transform.__init__(self, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 2050, in all\n",
      "    return arr.all(axis=axis, out=out, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 99, in __init__\n",
      "    self._parents = WeakValueDictionary()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/_methods.py\", line 40, in _all\n",
      "    def _all(a, axis=None, dtype=None, out=None, keepdims=False):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/weakref.py\", line 123, in __init__\n",
      "    self.update(*args, **kw)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/weakref.py\", line 288, in update\n",
      "    def update(*args, **kwargs):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2339, in autoscale_view\n",
      "    'minposy', self.yaxis, self._ymargin, y_stickies, self.set_ybound)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2319, in handle_single_axis\n",
      "    do_upper_margin = not np.any(np.isclose(x1, stickies))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/patches.py\", line 717, in _update_patch_transform\n",
      "    self._rect_transform = transforms.BboxTransformTo(bbox)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 2541, in isclose\n",
      "    return within_tol(x, y, atol, rtol)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 2524, in within_tol\n",
      "    result = less_equal(abs(x-y), atol + rtol * abs(y))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/matplotlib/transforms.py\", line 2561, in __init__\n",
      "    self._mtx = None\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f1ede2f54708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbeg_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m#     for i in tqdm_notebook(range(MAX_BATCH_SIZE)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effefb66c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "plt.figure(figsize=(20,140))\n",
    "\n",
    "SAVE_PATH = \"/disk3/yinghong/data/mobile-model/\"\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum(torch.pow(input - target,2)) / input.nelement()\n",
    "\n",
    "train_loss_all = []\n",
    "valid_loss_all = []\n",
    "# TBD(@yinghong): delete it after use\n",
    "def display_pose_ids(img, pose, ids):\n",
    "    print\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "#     pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "#     pose  = pose.numpy().reshape([-1,2])\n",
    "    pose  = pose.numpy().reshape([-1,2])\n",
    "#     img = img.cpu().numpy().transpose(1,2,0)\n",
    "#     img = img.numpy().transpose(1,2,0)\n",
    "    img = img.numpy().transpose(1,2,0)\n",
    "    colors = ['b', 'g', 'g', 'g', 'g', 'g', 'm', 'm', 'r', 'r', 'y', 'y', 'y', 'y','y','y']\n",
    "    pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n",
    "    colors_skeleton = ['r', 'y', 'y', 'g', 'g', 'y', 'y', 'g', 'g', 'm', 'm', 'g', 'g', 'y','y']\n",
    "\n",
    "    img = img*std+mean\n",
    "#     print(img.shape) # (227, 227, 3)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    plt.subplot(25,4,ids+1)\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(len(colors)):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color=colors[idx])\n",
    "    for idx in range(len(colors_skeleton)):\n",
    "        plt.plot(pose[pairs[idx],0], pose[pairs[idx],1],color=colors_skeleton[idx])\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=1))\n",
    "output_dict = {}\n",
    "\n",
    "MAX_BATCH_SIZE = 561\n",
    "IMG_WIDTH=227\n",
    "IMG_HEIGHT=227\n",
    "for epoch in tqdm_notebook(range(1000)):  # loop over the dataset multiple times\n",
    "    train_loss_epoch = []\n",
    "    beg_time = time.time()\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "#     for i in tqdm_notebook(range(MAX_BATCH_SIZE)):\n",
    "        if i % 20 == 0:\n",
    "            print(\"current=%d\" % i)\n",
    "            break\n",
    "    \n",
    "        # get the inputs\n",
    "       \n",
    "        images, poses = data['image'], data['pose']\n",
    "        print(\"poses\", poses)\n",
    "#         plt.figure(figsize=(20,140))\n",
    "#         print(images[0].shape)\n",
    "#         display_pose_ids(images[0], poses[0], 1)\n",
    "#         plt.show()\n",
    "#         obj = torch.load(SAVE_PATH + \"aug-data-same-scale-bz32-idx%d\" % i)\n",
    "#         images = obj['images']\n",
    "#         print(\"images.size,\", images[0])\n",
    "#         poses = obj['poses']\n",
    "#         print(\"before:\", images.shape, poses.shape)\n",
    "#         print(\"before:\", type(images), type(poses))\n",
    "   \n",
    "#         seqs = [iaa.Crop(px=(0, 16))]\n",
    "        \n",
    "###########data aug#############\n",
    "#         seqs = []\n",
    "#         for rotate in range(-180, 180, 45):\n",
    "#             seqs.append(iaa.Sequential([iaa.Affine(rotate=rotate)]))\n",
    "#         for scale_num in range(3, 8, 2):\n",
    "#             seqs.append(iaa.Sequential([iaa.Affine(scale=(scale_num / 10.0 , scale_num / 10.0 + 0.1))]))\n",
    "#         seqs.append(iaa.Sequential([iaa.Fliplr(1.0)]))\n",
    "# #         seqs.append(iaa.Sequential([iaa.Flipud(1.0)]))\n",
    "# #         seqs.append(iaa.GaussianBlur(sigma=(0, 1.0)))\n",
    "#         seqs.append(iaa.GaussianBlur(sigma=(0, 2.0)))\n",
    "# #         seqs.append(iaa.GaussianBlur(sigma=(0, 3.0)))\n",
    "\n",
    "#         seq_dets = [seq.to_deterministic() for seq in seqs] \n",
    "#         images, poses = img_augmentation(images, poses, seq_dets)\n",
    "#         print(\"after:\", images.shape, poses.shape)\n",
    "#         for i in range(4):\n",
    "#             print(\"begin drawing\")\n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[i], poses[i], i)\n",
    "#             plt.show()\n",
    "# #             print(poses[i])\n",
    "            \n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[24 + i], poses[24 + i], i)\n",
    "#             plt.show()\n",
    "#             print(poses[24 + i])\n",
    "            \n",
    "#             plt.figure(figsize=(20,140))\n",
    "#             display_pose(images[64 + i], poses[64 + i], i)\n",
    "#             plt.show()\n",
    "#             print(poses[128 + i])\n",
    "#         print(\"before:\", type(images), type(poses))\n",
    "\n",
    "#         if i == 0:\n",
    "# #             np.save(outfile, [images, poses])\n",
    "#             pickle.dump(output_dict, open(\"output_dict.txt\", \"wb\"))\n",
    "#             print(\"dump done\")\n",
    "\n",
    "        # wrap them in Variable\n",
    "        images, poses = Variable(images.cuda()), Variable(poses.cuda())\n",
    "        \n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        # TBD(@yinghong): change to a function\n",
    "#       zac  print(type(outputs[0][0]), type(poses[0][0]))\n",
    "#         print(outputs[0][0].data[0])\n",
    "\n",
    "#         for pose_idx in range(poses.size()[0]):\n",
    "#             x = poses[pose_idx][0].data[0]\n",
    "#             y = poses[pose_idx][1].data[0]\n",
    "#             if x < 0 or x < 0 or y >= IMG_WIDTH or y >= IMG_HEIGHT:\n",
    "#                 poses[pose_idx][0].data[0] = outputs[pose_idx][0].data[0]\n",
    "#                 poses[pose_idx][1].data[0] = outputs[pose_idx][1].data[0]\n",
    "        loss = criterion(outputs, poses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_epoch.append(loss.data[0])\n",
    "    print(\"epoch %d\" % epoch)\n",
    "    end_time = time.time()\n",
    "    if epoch%20==0:\n",
    "        PATH_PREFIX = '/disk3/yinghong/data/mobile-model/'\n",
    "        checkpoint_file = PATH_PREFIX + 'orig-checkpoint{}.t7'.format(1 + epoch)\n",
    "        torch.save(net, checkpoint_file)\n",
    "        valid_loss_epoch = []\n",
    "        for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "#             net_forward = torch.load(checkpoint_file).cuda(device_id=gpus[0])\n",
    "            net_forward = torch.load(checkpoint_file).cuda()\n",
    "            images = sample_batched['image'].cuda()\n",
    "            poses = sample_batched['pose'].cuda()\n",
    "            outputs = net_forward(Variable(images, volatile=True))\n",
    "            valid_loss_epoch.append(mse_loss(outputs.data,poses))\n",
    "        print('[epoch %d] train loss: %.8f, valid loss: %.8f' %\n",
    "          (epoch + 1, sum(train_loss_epoch)/(71*256), sum(valid_loss_epoch)/(8*256)))\n",
    "        print('==> checkpoint model saving to %s'%checkpoint_file)\n",
    "        train_loss_all.append(sum(train_loss_epoch)/(71*256))\n",
    "        valid_loss_all.append(sum(valid_loss_epoch)/(8*256))\n",
    "    print(\"Time:%d\", end_time - beg_time)\n",
    "            \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dict = pickle.load(open(\"output_dict.txt\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_dict['images'].shape)\n",
    "# print(output_dict['poses'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# output_dict = pickle.load(open(\"output_dict.txt\", \"rb\"))\n",
    "# print(output_dict['images'].shape)\n",
    "# print(output_dict['poses'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_dict['poses'].numpy()[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# x1 = torch.zeros(10, 10)\n",
    "# x2 = x1.unsqueeze(0)\n",
    "# print(x2.size())\n",
    "# sz=torch.Size([1, 10, 10])\n",
    "# print(sz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
