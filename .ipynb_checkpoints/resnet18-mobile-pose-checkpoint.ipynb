{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "# from multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "torch.cuda.set_device(0) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "def display_pose(img, pose):\n",
    "    pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "    img = img.cpu().numpy().transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "\n",
    "        h, w = image_.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image = transform.resize(image_, (new_h, new_w))\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = torch.from_numpy(((image-mean)/std).transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/lsp_ext\"\n",
    "\n",
    "train_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'train_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256,\n",
    "                        shuffle=False, num_workers = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n",
    "\n",
    "# net = Net().cuda(device_id=gpus[0])\n",
    "\n",
    "    \n",
    "net = torch.load('/home/yuliang/code/DeepPose-pytorch-yh/checkpoint/checkpoint20.t7').cuda(device_id=gpus[0])\n",
    "criterion = nn.MSELoss().cuda()\n",
    "\n",
    "def weighted_mse_loss(input, target, weights):\n",
    "    out = (input - target) * (input - target) \n",
    "    out = out * weights.expand_as(out)\n",
    "    # expand_as because weights are prob not defined for mini-batch\n",
    "    loss = out.sum() / input.nelement() # or sum over whatever dimensions\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(input, target, weights):\n",
    "    out = torch.abs(input - target)\n",
    "    out = out * weights.expand_as(out)\n",
    "    # expand_as because weights are prob not defined for mini-batch\n",
    "    loss = out.sum() / input.nelement() # or sum over whatever dimensions\n",
    "    return loss\n",
    "\n",
    "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed61ff4282a54e0cb917b20a786e625e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea417a75d84e41558159c12ea46a6bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "[epoch 1] train loss: 8.72473311, valid loss: 4.33615003\n",
      "==> checkpoint model saving to lsp-l2-checkpoint0.t7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb4cd92846940d8af323c87b957b5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-48:\n",
      "Process Process-44:\n",
      "Process Process-41:\n",
      "Exception ignored in: <bound method Image.__del__ of <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=217x217 at 0x7F6DB6401438>>\n",
      "Process Process-43:\n",
      "Process Process-42:\n",
      "Traceback (most recent call last):\n",
      "Process Process-46:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 571, in __del__\n",
      "Process Process-49:\n",
      "Process Process-45:\n",
      "Process Process-47:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-504c26f73a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#         print(\"loss,\", loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#         print(\"loss.data[0],\", loss.data[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lsp-l2-checkpoint{}.t7'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    def __del__(self):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 131, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2968)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 65, in __getitem__\n",
      "    box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-2-798521f6a815>\", line 7, in expand_bbox\n",
      "    new_top = np.clip(top-ratio*height,0,img_height)\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 131, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2968)\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 131, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2968)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 1727, in clip\n",
      "    return _wrapfunc(a, 'clip', a_min, a_max, out=out)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 35, in __call__\n",
      "    image = torch.from_numpy(((image-mean)/std).transpose((2, 0, 1))).float()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 585, in warp\n",
      "    def warp(image, inverse_map, map_args={}, output_shape=None, order=1,\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 131, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2968)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 131, in resize\n",
      "    tform.estimate(src_corners, dst_corners)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 688, in estimate\n",
      "    H = np.dot(np.linalg.inv(dst_matrix), np.dot(H, src_matrix))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 514, in inv\n",
      "    return wrap(ainv.astype(result_t, copy=False))\n",
      "KeyboardInterrupt\n",
      "Process Process-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 71, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-a4d25a8b9b5d>\", line 21, in __call__\n",
      "    image = transform.resize(image_, (new_h, new_w))\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 775, in warp\n",
      "    order=order, mode=mode, cval=cval))\n",
      "  File \"skimage/transform/_warps_cy.pyx\", line 131, in skimage.transform._warps_cy._warp_fast (skimage/transform/_warps_cy.c:2968)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum(torch.pow(input - target,2)) / input.nelement()\n",
    "\n",
    "# mpii to lsp\n",
    "lsp_part_names = ['RAnkle', 'RKnee', 'RHip', 'LHip', 'LKnee', 'LAnkle', 'RWrist',  \n",
    "              'RElbow', 'RShoulder', 'LShoulder', 'LElbow', 'LWrist', 'Neck', 'Head']\n",
    "mpii_part_names = ['RAnkle', 'RKnee', 'RHip', 'LHip', 'LKnee', 'LAnkle', 'Pelv', \n",
    "                   'Thrx', 'Neck', 'Head', 'RWrist', 'RElbow', 'RShoulder', 'LShoulder',\n",
    "                   'LElbow', 'LWrist']\n",
    "idx_arr = []\n",
    "MATCHES = 0\n",
    "for name in lsp_part_names:\n",
    "    if  name in mpii_part_names:\n",
    "        pos = mpii_part_names.index(name)\n",
    "        idx_arr.append(pos * 2)\n",
    "        idx_arr.append(pos * 2 + 1)\n",
    "        MATCHES += 1\n",
    "\n",
    "train_loss_all = []\n",
    "valid_loss_all = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(1000)):  # loop over the dataset multiple times\n",
    "    \n",
    "    train_loss_epoch = []\n",
    "    for i, data in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        print (i)\n",
    "        # get the inputs\n",
    "        images, poses = data['image'], data['pose']\n",
    "        # wrap them in Variable\n",
    "        images, poses = Variable(images.cuda()), Variable(poses.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "\n",
    "        # add sigmas\n",
    "        sigmas = np.array([0.089,  0.087,  0.107,  0.107,  0.087,  0.089,\n",
    "                   0.107,  0.107,  0.035,  0.035,  0.062,  0.072,\n",
    "                   0.079,  0.079,  0.072,  0.062])\n",
    "        sigmas = np.array([ (val, val) for val in sigmas]).flatten()\n",
    "        vars = 1.0 / (sigmas * 2)**2\n",
    "        vars /= np.mean(vars)\n",
    "#         loss = weighted_mse_loss(outputs, poses, Variable(torch.from_numpy(vars).float()).cuda())\n",
    "        # mpii to lsp\n",
    "        poses = poses[:MATCHES * 2]\n",
    "        outputs = outputs[:, idx_arr][:MATCHES * 2]\n",
    "        \n",
    "        loss = criterion(outputs, poses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "#         print(\"loss,\", loss)\n",
    "#         print(\"loss.data[0],\", loss.data[0])\n",
    "        train_loss_epoch.append(loss.data[0])\n",
    "    if epoch%5==0:\n",
    "        checkpoint_file = 'lsp-l2-checkpoint{}.t7'.format(epoch)\n",
    "        torch.save(net, checkpoint_file)\n",
    "        valid_loss_epoch = []\n",
    "        for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "\n",
    "            net_forward = torch.load(checkpoint_file).cuda(device_id=gpus[0])\n",
    "            images = sample_batched['image'].cuda(device=gpus[0])\n",
    "            poses = sample_batched['pose'].cuda(device=gpus[0])\n",
    "            outputs = net_forward(Variable(images, volatile=True))\n",
    "            \n",
    "            poses = poses[:MATCHES * 2]\n",
    "            outputs = outputs[:, idx_arr][:MATCHES * 2]\n",
    "        \n",
    "            valid_loss_epoch.append(mse_loss(outputs.data,poses))\n",
    "        print('[epoch %d] train loss: %.8f, valid loss: %.8f' %\n",
    "          (epoch + 1, sum(train_loss_epoch)/(71*256), sum(valid_loss_epoch)/(8*256)))\n",
    "        print('==> checkpoint model saving to %s'%checkpoint_file)\n",
    "        train_loss_all.append(sum(train_loss_epoch)/(71*256))\n",
    "        valid_loss_all.append(sum(valid_loss_epoch)/(8*256))\n",
    "            \n",
    "  \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "8\n",
      "9\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 16, 17, 18, 19]\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "    0    -1    -2    -3    -4    -5    -6    -7    -8    -9   -10   -11   -12\n",
      "\n",
      "Columns 13 to 25 \n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "  -13   -14   -15   -16   -17   -18   -19   -20   -21   -22   -23   -24   -25\n",
      "\n",
      "Columns 26 to 27 \n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "  -26   -27\n",
      "[torch.FloatTensor of size 24x28]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "    0     1     2     3     4     5     6     7     8     9    10    11    20\n",
      "\n",
      "Columns 13 to 25 \n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "   21    22    23    24    25    26    27    28    29    30    31    16    17\n",
      "\n",
      "Columns 26 to 27 \n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "   18    19\n",
      "[torch.FloatTensor of size 24x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = torch.Tensor(24, 32)\n",
    "poses = torch.Tensor(24, 28)\n",
    "for i in range(24):\n",
    "    for j in range(32):\n",
    "        output[i][j] = j\n",
    "        \n",
    "for i in range(24):\n",
    "    for j in range(28):\n",
    "        poses[i][j] = -j\n",
    "        \n",
    "lsp_part_names = ['RAnkle', 'RKnee', 'RHip', 'LHip', 'LKnee', 'LAnkle', 'RWrist',  \n",
    "                  'RElbow', 'RShoulder', 'LShoulder', 'LElbow', 'LWrist', 'Neck', 'Head']\n",
    "mpii_part_names = ['RAnkle', 'RKnee', 'RHip', 'LHip', 'LKnee', 'LAnkle', 'Pelv', \n",
    "                   'Thrx', 'Neck', 'Head', 'RWrist', 'RElbow', 'RShoulder', 'LShoulder',\n",
    "                   'LElbow', 'LWrist']\n",
    "idx_arr = []\n",
    "matches = 0\n",
    "for name in lsp_part_names:\n",
    "    if  name in mpii_part_names:\n",
    "        pos = mpii_part_names.index(name)\n",
    "        print(pos)\n",
    "        idx_arr.append(pos * 2)\n",
    "        idx_arr.append(pos * 2 + 1)\n",
    "        matches += 1\n",
    "\n",
    "print(idx_arr)\n",
    "\n",
    "print(poses[:matches * 2])\n",
    "output = output[:, idx_arr][:matches * 2]\n",
    "print(output)\n",
    "\n",
    "# print(lsp_part_names[:matches])\n",
    "# for i in range(len(mpii_part_names)):\n",
    "#     if not i in idx_arr:\n",
    "#         idx_arr.append(i)\n",
    "# print(idx_arr)\n",
    "# print(len(mpii_part_names))\n",
    "# print(idx_arr)\n",
    "# print(mpii_part_names[:,idx_arr])\n",
    "\n",
    "# poses = torch.Tensor(24, 28).fill_(-1)\n",
    "# print(output[:, :28])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f3aa91c584ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mna\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \"\"\"\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "na = np.array(a)\n",
    "print(na)\n",
    "np.transpose(na,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((1, 2, 3))\n",
    "print(x.shape)\n",
    "np.transpose(x, (1, 0, 2)).shape\n",
    "(2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 3\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "y = torch.randn(3, 2, 5)\n",
    "ix = torch.LongTensor([1, 3])\n",
    "print(ix)\n",
    "x[:, ix, :] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
