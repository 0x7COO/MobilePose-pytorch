{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "# from multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# torch.cuda.set_device(1) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "def display_pose(img, pose):\n",
    "    pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "    img = img.cpu().numpy().transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "        h, w = image_.shape[:2]\n",
    "        im_scale = min(float(self.output_size[0]) / float(h), float(self.output_size[1]) / float(w))\n",
    "        new_h = int(image_.shape[0] * im_scale)\n",
    "        new_w = int(image_.shape[1] * im_scale)\n",
    "        image = cv2.resize(image_, (new_w, new_h),\n",
    "                    interpolation=cv2.INTER_LINEAR)\n",
    "        left_pad = (self.output_size[1] - new_w) // 2\n",
    "        right_pad = (self.output_size[1] - new_w) - left_pad\n",
    "        top_pad = (self.output_size[0] - new_h) // 2\n",
    "        bottom_pad = (self.output_size[0] - new_h) - top_pad\n",
    "        mean=np.array([0.485, 0.456, 0.406]) * 256\n",
    "        pad = ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "        image = np.stack([np.pad(image[:,:,c], pad, mode='constant', constant_values=mean[c]) \n",
    "                        for c in range(3)], axis=2)\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h]))\n",
    "        pose += [right_pad, top_pad]\n",
    "        pose = pose.flatten()\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         image_, pose_ = sample['image'], sample['pose']\n",
    "\n",
    "#         h, w = image_.shape[:2]\n",
    "#         if isinstance(self.output_size, int):\n",
    "#             if h > w:\n",
    "#                 new_h, new_w = self.output_size * h / w, self.output_size\n",
    "#             else:\n",
    "#                 new_h, new_w = self.output_size, self.output_size * w / h\n",
    "#         else:\n",
    "#             new_h, new_w = self.output_size\n",
    "\n",
    "#         new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "#         image = transform.resize(image_, (new_h, new_w))\n",
    "#         pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "#         return {'image': image, 'pose': pose}\n",
    "\n",
    "class Expansion(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    "        h, w = image.shape[:2]\n",
    "        x = np.arange(0, h)\n",
    "        y = np.arange(0, w) \n",
    "        x, y = np.meshgrid(x, y)\n",
    "        x = x[:,:, np.newaxis]\n",
    "        y = y[:,:, np.newaxis]\n",
    "        image = np.concatenate((image, x), axis=2)\n",
    "        image = np.concatenate((image, y), axis=2)\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "    \n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = (image[:,:,:3]-mean)/std\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "\n",
    "train_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'train_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               #Expansion(),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               #Expansion(),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                        shuffle=False, num_workers = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n",
    "\n",
    "# net = torch.load('checkpoint20.t7').cuda(device_id=gpus[1])\n",
    "net = Net()\n",
    "criterion = nn.MSELoss().cuda()\n",
    "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e75fc39e1514e9ca6cec5910c2be669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 in train_dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "KeyboardInterrupt\n",
      "Process Process-6:\n",
      "Process Process-1:\n",
      "Process Process-9:\n",
      "Process Process-4:\n",
      "Process Process-10:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-8:\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 74, in __call__\n",
      "    image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 95, in __getitem__\n",
      "    image = io.imread(img_path)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 36, in imread\n",
      "    im = Image.open(f)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 2506, in open\n",
      "    im = _open_core(fp, filename, prefix)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/tensor.py\", line 43, in float\n",
      "    return self.type(type(self).__module__ + '.FloatTensor')\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torch/_utils.py\", line 35, in _type\n",
      "    return new_type(self.size()).copy_(self, async)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 2496, in _open_core\n",
      "    im = factory(fp, filename)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 110, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 37, in imread\n",
      "    return pil_to_ndarray(im, dtype=dtype, img_num=img_num)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 53, in pil_to_ndarray\n",
      "    im.getdata()[0]\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/Image.py\", line 1187, in getdata\n",
      "    self.load()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/torchvision/transforms.py\", line 34, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-4-f030c3a7127b>\", line 24, in __call__\n",
      "    pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h]))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 757, in jpeg_factory\n",
      "    im = JpegImageFile(fp, filename)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/ImageFile.py\", line 100, in __init__\n",
      "    self._open()\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 335, in _open\n",
      "    handler(self, i)\n",
      "  File \"/home/yuliang/.pyenv/versions/3.6.2/envs/env3.6/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 65, in APP\n",
      "    app = \"APP%d\" % (marker & 15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a567896db7ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mPATH_PREFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/disk3/yinghong/data/mobile-model/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum(torch.pow(input - target,2)) / input.nelement()\n",
    "\n",
    "train_loss_all = []\n",
    "valid_loss_all = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(1000)):  # loop over the dataset multiple times\n",
    "    \n",
    "    train_loss_epoch = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        if i % 20 == 0:\n",
    "            print (\"i=%d in train_dataloader\" % i)\n",
    "        # get the inputs\n",
    "        images, poses = data['image'], data['pose']\n",
    "        # wrap them in Variable\n",
    "        images, poses = Variable(images.cuda()), Variable(poses.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, poses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_epoch.append(loss.data[0])\n",
    "    if epoch%10==0:\n",
    "        PATH_PREFIX = '/disk3/yinghong/data/mobile-model/'\n",
    "        checkpoint_file = PATH_PREFIX + 'scale2-checkpoint{}.t7'.format(epoch)\n",
    "#         checkpoint_file = 'checkpoint{}.t7'.format(epoch)\n",
    "        torch.save(net, checkpoint_file)\n",
    "        valid_loss_epoch = []\n",
    "        for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "\n",
    "            net_forward = torch.load(checkpoint_file).cuda()\n",
    "            images = sample_batched['image'].cuda()\n",
    "            poses = sample_batched['pose'].cuda()\n",
    "            outputs = net_forward(Variable(images, volatile=True))\n",
    "            valid_loss_epoch.append(mse_loss(outputs.data,poses))\n",
    "        print('[epoch %d] train loss: %.8f, valid loss: %.8f' %\n",
    "          (epoch + 1, sum(train_loss_epoch)/(71*256), sum(valid_loss_epoch)/(8*256)))\n",
    "        print('==> checkpoint model saving to %s'%checkpoint_file)\n",
    "        train_loss_all.append(sum(train_loss_epoch)/(71*256))\n",
    "        valid_loss_all.append(sum(valid_loss_epoch)/(8*256))\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
