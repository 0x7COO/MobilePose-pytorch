{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "# from multiprocessing import set_start_method\n",
    "# try:\n",
    "#     set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "torch.cuda.set_device(0) \n",
    "torch.backends.cudnn.enabled = True\n",
    "print(torch.cuda.device_count())\n",
    "gpus = [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(left, right, top, bottom, img_width, img_height):\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    ratio = 0.15\n",
    "    new_left = np.clip(left-ratio*width,0,img_width)\n",
    "    new_right = np.clip(right+ratio*width,0,img_width)\n",
    "    new_top = np.clip(top-ratio*height,0,img_height)\n",
    "    new_bottom = np.clip(bottom+ratio*height,0,img_height)\n",
    "    return [int(new_left), int(new_top), int(new_right), int(new_bottom)]\n",
    "\n",
    "def display_pose(img, pose):\n",
    "    pose  = pose.data.cpu().numpy().reshape([-1,2])\n",
    "    img = img.cpu().numpy().transpose(1,2,0)\n",
    "    img_width, img_height,_ = img.shape\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for idx in range(16):\n",
    "        plt.plot(pose[idx,0], pose[idx,1], marker='o', color='yellow')\n",
    "    xmin = np.min(pose[:,0])\n",
    "    ymin = np.min(pose[:,1])\n",
    "    xmax = np.max(pose[:,0])\n",
    "    ymax = np.max(pose[:,1])\n",
    "    bndbox = np.array(expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height))\n",
    "    coords = (bndbox[0], bndbox[1]), bndbox[2]-bndbox[0]+1, bndbox[3]-bndbox[1]+1\n",
    "    ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor='yellow', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collisions detected in /home/yuliang/.jupyter/jupyter_notebook_config.py and /home/yuliang/.jupyter/jupyter_notebook_config.json config files. /home/yuliang/.jupyter/jupyter_notebook_config.json has higher priority: {\n",
      "      \"NotebookApp\": {\n",
      "        \"password\": \"'' ignored, using 'sha1:3090b69b92d4:448f04cba181daed83957ad1568f2407c7af7f56'\"\n",
      "      }\n",
      "    }\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image_, pose_ = sample['image'], sample['pose']\n",
    "\n",
    "        h, w = image_.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image = transform.resize(image_, (new_h, new_w))\n",
    "        pose = (pose_.reshape([-1,2])/np.array([w,h])*np.array([new_w,new_h])).flatten()\n",
    "        return {'image': image, 'pose': pose}\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pose = sample['image'], sample['pose']\n",
    " \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mean=np.array([0.485, 0.456, 0.406])\n",
    "        std=np.array([0.229, 0.224, 0.225])\n",
    "        image = torch.from_numpy(((image-mean)/std).transpose((2, 0, 1))).float()\n",
    "        pose = torch.from_numpy(pose).float()\n",
    "        \n",
    "        return {'image': image,\n",
    "                'pose': pose}\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform):\n",
    "        \n",
    "        with open(csv_file) as f:\n",
    "            self.f_csv = list(csv.reader(f, delimiter='\\t'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "        line = self.f_csv[idx][0].split(\",\")\n",
    "        img_path = os.path.join(ROOT_DIR,'images',line[0])\n",
    "        image = io.imread(img_path)\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        pose = np.array([float(item) for item in line[1:]]).reshape([-1,2])\n",
    "        \n",
    "        xmin = np.min(pose[:,0])\n",
    "        ymin = np.min(pose[:,1])\n",
    "        xmax = np.max(pose[:,0])\n",
    "        ymax = np.max(pose[:,1])\n",
    "        \n",
    "        box = expand_bbox(xmin, xmax, ymin, ymax, width, height)\n",
    "        image = image[box[1]:box[3],box[0]:box[2],:]\n",
    "        pose = (pose-np.array([box[0],box[1]])).flatten()\n",
    "        \n",
    "        sample = {'image': image, 'pose':pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/yuliang/code/deeppose_tf/datasets/mpii\"\n",
    "\n",
    "train_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'train_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256,\n",
    "                        shuffle=False, num_workers = 10)\n",
    "\n",
    "test_dataset = PoseDataset(csv_file=os.path.join(ROOT_DIR,'test_joints.csv'),\n",
    "                                  transform=transforms.Compose([\n",
    "                                               Rescale((227,227)),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256,\n",
    "                        shuffle=False, num_workers = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        model.fc=nn.Linear(512,32)\n",
    "        self.resnet = model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        pose_out = self.resnet(x)\n",
    "        return pose_out\n",
    "\n",
    "net = torch.load('checkpoint20.t7').cuda(device_id=gpus[1])\n",
    "criterion = nn.MSELoss().cuda()\n",
    "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a63ed6c91ea47a4915cd1db1707580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train loss: 1.07181829, valid loss: 2.38582148\n",
      "==> checkpoint model saving to checkpoint0.t7\n",
      "[epoch 21] train loss: 0.37634598, valid loss: 2.28163813\n",
      "==> checkpoint model saving to checkpoint20.t7\n",
      "[epoch 41] train loss: 0.20067885, valid loss: 2.30869816\n",
      "==> checkpoint model saving to checkpoint40.t7\n",
      "[epoch 61] train loss: 0.12066363, valid loss: 2.30368282\n",
      "==> checkpoint model saving to checkpoint60.t7\n",
      "[epoch 81] train loss: 0.08223151, valid loss: 2.34044272\n",
      "==> checkpoint model saving to checkpoint80.t7\n",
      "[epoch 101] train loss: 0.05723832, valid loss: 2.28938123\n",
      "==> checkpoint model saving to checkpoint100.t7\n",
      "[epoch 121] train loss: 0.05272155, valid loss: 2.23591500\n",
      "==> checkpoint model saving to checkpoint120.t7\n",
      "[epoch 141] train loss: 0.03236602, valid loss: 2.23349110\n",
      "==> checkpoint model saving to checkpoint140.t7\n",
      "[epoch 161] train loss: 0.03009566, valid loss: 2.24053067\n",
      "==> checkpoint model saving to checkpoint160.t7\n",
      "[epoch 181] train loss: 0.02156745, valid loss: 2.26069921\n",
      "==> checkpoint model saving to checkpoint180.t7\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def mse_loss(input, target):\n",
    "    return torch.sum(torch.pow(input - target,2)) / input.nelement()\n",
    "\n",
    "train_loss_all = []\n",
    "valid_loss_all = []\n",
    "\n",
    "for epoch in tqdm_notebook(range(1000)):  # loop over the dataset multiple times\n",
    "    \n",
    "    train_loss_epoch = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs\n",
    "        images, poses = data['image'], data['pose']\n",
    "        # wrap them in Variable\n",
    "        images, poses = Variable(images.cuda()), Variable(poses.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, poses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_epoch.append(loss.data[0])\n",
    "    if epoch%20==0:\n",
    "        checkpoint_file = 'checkpoint{}.t7'.format(epoch)\n",
    "        torch.save(net, checkpoint_file)\n",
    "        valid_loss_epoch = []\n",
    "        for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "\n",
    "            net_forward = torch.load(checkpoint_file).cuda(device_id=gpus[0])\n",
    "            images = sample_batched['image'].cuda(device=gpus[0])\n",
    "            poses = sample_batched['pose'].cuda(device=gpus[0])\n",
    "            outputs = net_forward(Variable(images, volatile=True))\n",
    "            valid_loss_epoch.append(mse_loss(outputs.data,poses))\n",
    "        print('[epoch %d] train loss: %.8f, valid loss: %.8f' %\n",
    "          (epoch + 1, sum(train_loss_epoch)/(71*256), sum(valid_loss_epoch)/(8*256)))\n",
    "        print('==> checkpoint model saving to %s'%checkpoint_file)\n",
    "        train_loss_all.append(sum(train_loss_epoch)/(71*256))\n",
    "        valid_loss_all.append(sum(valid_loss_epoch)/(8*256))\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
